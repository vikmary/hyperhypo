{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import BertTokenizer\n",
    "from dataset import get_hypernyms_list_from_train, HypoDataset\n",
    "\n",
    "\n",
    "data_path = Path('/home/hdd/data/hypernym/')\n",
    "corpus_path = data_path / 'corpus.news_dataset-sample.token.txt'\n",
    "hypo_index_path = data_path / 'index.full.news_dataset-sample.json'\n",
    "train_set_path = data_path / 'train.cased.json'\n",
    "candidates_path = data_path / 'candidates.cased.tsv'\n",
    "wordnet_path = Path('/home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data')\n",
    "\n",
    "model_path = Path('/home/hdd/models/rubert_v2/rubert_cased_L-12_H-768_A-12_v2/')\n",
    "tokenizer_vocab_path = model_path / 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(tokenizer_vocab_path, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_hype_list = get_hypernyms_list_from_train(train_set_path)\n",
    "\n",
    "# ds = HypoDataset(tokenizer,\n",
    "#                  corpus_path,\n",
    "#                  hypo_index_path,\n",
    "#                  train_set_path,\n",
    "#                  train_hype_list)\n",
    "\n",
    "# corpus = df.corpus\n",
    "# index = ds.hypo_index[hypo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = ds.train_set[23]\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypo = sample[0][0]\n",
    "# hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading index from /home/hdd/data/hypernym/index.full.news_dataset-sample.json.\n",
      "Loading corpus from /home/hdd/data/hypernym/corpus.news_dataset-sample.token.txt.\n"
     ]
    }
   ],
   "source": [
    "from corpus_indexed import CorpusIndexed\n",
    "\n",
    "corpus_indexed = CorpusIndexed(hypo_index_path)\n",
    "\n",
    "index = corpus_indexed.idx\n",
    "corpus = corpus_indexed.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore predictions for specific hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo = 'школьный учитель'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypo in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t ['школьного', 'учителя']\n",
      "Соколовская : Размер оклада школьного учителя должен составлять 70 % от всей заработной платы Размер оклада школьного учителя должен составлять 70 % от всей заработной платы .\n",
      "2 \t ['школьного', 'учителя']\n",
      "Соколовская : Размер оклада школьного учителя должен составлять 70 % от всей заработной платы Размер оклада школьного учителя должен составлять 70 % от всей заработной платы .\n",
      "3 \t ['школьных', 'учителей']\n",
      "При обсуждении вопросов заработной платы и нагрузки на школьных учителей необходимо в первую очередь обратиться к федеральному закону о труде , в котором говорится , что должностной оклад – это фиксированный размер оплаты труда работника за исполнение должностных обязанностей без учета компенсационных , стимулирующих и социальных выплат .\n",
      "4 \t ['школьного', 'учителя']\n",
      "Первыми Героями СССР были летчики М . В . Водопьянов , И . В . Доронин , Н . П . Каманин , А . В . Ляпидевский ( наш земляк родом из села Белая Глина , сын школьного учителя .\n",
      "5 \t ['школьных', 'учителей']\n",
      "Как сообщили в пресс - службе министерства экономики и территориального развития Свердловской области , за время реализации « майских » указов Президента РФ по сравнению с 2011 годом зарплата школьных учителей , врачей и среднего медперсонала выросла в 1 , 5 раза , педагогических работников учреждений дошкольного и дополнительного образования — в 2 , 3 раза , младшего медперсонала и педагогов среднего профобразования — в 1 , 95 раза , сотрудников учреждений культуры — в 2 , 5 раза , социальных работников — в 1 , 8 раза .\n",
      "6 \t ['школьные', 'учителя']\n",
      "Средние зарплаты бюджетников по итогам 2016 года : школьные учителя — 32955 руб . , педагоги из учреждений дополнительного образования детей — 30388 руб . , воспитатели детсадов — 29991 руб . , педагоги среднего профобразования — 28548 руб . , средний медперсонал — 28251 руб . , сотрудники учреждений культуры — 26637 руб . , социальные работники — 23088 руб . ,\n",
      "7 \t ['школьного', 'учителя']\n",
      "И еще накануне заявляли , что действия школьного учителя соответствуют всем необходимым нормам .\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for corp_id, start_idx, end_idx in index[hypo]:\n",
    "    count += 1\n",
    "    text = corpus[corp_id]\n",
    "    \n",
    "    print(count, '\\t', text.split()[start_idx: end_idx])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedder import get_word_embeddings\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(model_path / 'bert_config.json')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=False)\n",
    "model = BertModel.from_pretrained(str(model_path / 'ptrubert.pt'), config=config)\n",
    "\n",
    "emb_mat = model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing /home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data/synsets.N.xml.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_wordnet_synsets, enrich_with_wordnet_relations, synsets2senses\n",
    "\n",
    "\n",
    "wordnet_synsets = get_wordnet_synsets(wordnet_path.glob('synsets.N*'))\n",
    "# wordnet_synsets = get_wordnet_synsets(wordnet_path.glob('synsets.*'))\n",
    "# enrich_with_wordnet_relations(wordnet_synsets, wordnet_path.glob('synset_relations.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernym embeddings are of shape torch.Size([29296, 768])\n"
     ]
    }
   ],
   "source": [
    "LEVEL = 'sense'\n",
    "\n",
    "if LEVEL == 'synset':\n",
    "    # embedding all synsets (list of phrases)\n",
    "    hype_embs, hype_list = [], []\n",
    "    for s_id, synset in wordnet_synsets.items():\n",
    "        if s_id[-1] != 'N':\n",
    "            print(f'skipping {s_id}')\n",
    "        hype_list.append(synset['ruthes_name'].lower())\n",
    "        senses = [sense['content'].lower() for sense in synset['senses']]\n",
    "        hype_embs.append(get_word_embeddings(senses, emb_mat, tokenizer=tokenizer).mean(dim=0))\n",
    "    hype_embs = torch.stack(hype_embs, dim=0)\n",
    "elif LEVEL == 'sense':\n",
    "    # embedding all senses (phrases)\n",
    "    hype_list = sorted(set(synset['ruthes_name'].lower()\n",
    "                           for s_id, synset in wordnet_synsets.items()\n",
    "                           if s_id[-1] == 'N'))\n",
    "    hype_embs = get_word_embeddings(hype_list, emb_mat, tokenizer=tokenizer)\n",
    "print(f\"Hypernym embeddings are of shape {hype_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "h norm = tensor([[12.6992, 16.4481, 14.2615, 13.5348, 17.9549, 18.5590, 18.9328, 15.3944,\n",
      "         17.7733, 18.5994, 18.4554, 18.4417, 18.6928, 18.7628, 19.0013, 17.8019,\n",
      "         18.4879, 17.7364, 18.3655, 19.0120, 14.6933, 16.9564, 18.1769, 18.5649,\n",
      "         18.6077, 18.7214, 18.2263, 18.9732, 17.5855, 18.7139, 11.4048, 11.7314]],\n",
      "       grad_fn=<NormBackward3>)\n"
     ]
    }
   ],
   "source": [
    "NUM_CONTEXTS = 1\n",
    "\n",
    "\n",
    "token_idxs, hypo_masks = [], []\n",
    "max_len = 0\n",
    "\n",
    "for i, (corp_id, start_idx, end_idx) in enumerate(index[hypo]):\n",
    "    text = corpus[corp_id]\n",
    "    \n",
    "    subtokens, mask = ['[CLS]'], [0.0]\n",
    "    for n, token in enumerate(text.split()):\n",
    "        current_subtokens = tokenizer.tokenize(token)\n",
    "        subtokens.extend(current_subtokens)\n",
    "        mask_val = 1.0 if n in range(start_idx, end_idx) else 0.0\n",
    "        mask.extend([mask_val] * len(current_subtokens))\n",
    "    subtokens.append('[SEP]')\n",
    "    print(sum(mask))\n",
    "    mask.append(0.0)\n",
    "    \n",
    "    token_idxs.append(tokenizer.convert_tokens_to_ids(subtokens))\n",
    "    hypo_masks.append(mask)\n",
    "    max_len = max_len if max_len > len(subtokens) else len(subtokens)\n",
    "    if i >= NUM_CONTEXTS - 1:\n",
    "        break\n",
    "    \n",
    "# pad to max_len\n",
    "attn_masks = [[1.0] * len(idxs) + [0.0] * (max_len - len(idxs)) for idxs in token_idxs]\n",
    "token_idxs = [idxs + [0] * (max_len - len(idxs)) for idxs in token_idxs]\n",
    "hypo_masks = [mask + [0.0] * (max_len - len(mask)) for mask in hypo_masks]\n",
    "    \n",
    "# h: [batch_size, seq_len, hidden_size]\n",
    "h, v = model(torch.tensor(token_idxs), attention_mask=torch.tensor(attn_masks))\n",
    "print('h norm =', h.norm(dim=2))\n",
    "# hypo_mask_t: [batch_size, seq_len]\n",
    "hypo_mask_t = torch.tensor(hypo_masks)\n",
    "# r: [batch_size, seq_len, hidden_size]\n",
    "r = h * hypo_mask_t.unsqueeze(2)\n",
    "# r: [batch_size, hidden_size]\n",
    "r = r.sum(dim=1) / hypo_mask_t.sum(dim=1, keepdim=True)\n",
    "# # r: [hidden_size]\n",
    "# r = r.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'cosine'\n",
    "\n",
    "# scores: [batch_size, vocab_size]\n",
    "if METRIC == 'cosine':\n",
    "    # hype_embs_norm: [vocab_size, hidden_size]\n",
    "    hype_embs_norm = hype_embs / hype_embs.norm(dim=1, keepdim=True)\n",
    "    scores = r @ hype_embs_norm.T / r.norm(dim=1)\n",
    "elif METRIC == 'product':\n",
    "    scores = r @ hype_embs.T\n",
    "    \n",
    "scores = torch.log_softmax(scores, dim=1)\n",
    "# scores: [vocab_size]\n",
    "scores = scores.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, indices = torch.topk(scores, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), tensor(15.1952, grad_fn=<NormBackward0>))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape, r.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4623, grad_fn=<NormBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 6154\n",
    "\n",
    "hype_embs[idx].norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5005, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hype_embs[idx] * r).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'учитель' in hype_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "школьный учитель          -10.1      28423\n",
      "школьник                  -10.2      28414\n",
      "школьный урок             -10.2      28422\n",
      "преодолеть                -10.2      19067\n",
      "организованный преступник -10.2      15550\n",
      "преступник                -10.2      19120\n",
      "школьный предмет          -10.2      28421\n",
      "школьное обучение         -10.2      28418\n",
      "учитель физкультуры       -10.2      26714\n",
      "карат                     -10.2       8950\n",
      "преподаватель             -10.2      19073\n",
      "алфавит                   -10.2        547\n",
      "вельск                    -10.2       2657\n",
      "эльф                      -10.2      28909\n",
      "ушица                     -10.2      26731\n",
      "школьное изложение        -10.2      28417\n",
      "онега                     -10.2      15317\n",
      "ядро процессора           -10.2      29203\n",
      "покрыть поверхность       -10.2      18109\n",
      "ассамблея                 -10.2        967\n",
      "нагорье                   -10.2      13299\n",
      "начальные классы          -10.2      13753\n",
      "истина, правда            -10.2       8575\n",
      "школьный завуч            -10.2      28420\n",
      "шалфей                    -10.2      28213\n",
      "ящерица                   -10.2      29293\n",
      "тесьма                    -10.2      25262\n",
      "протестант                -10.2      19983\n",
      "олифа                     -10.2      15290\n",
      "одноклассник              -10.2      15125\n",
      "частный учитель           -10.2      27890\n",
      "пристрелка прицела        -10.2      19471\n",
      "наследство                -10.2      13626\n",
      "спица колеса              -10.2      23824\n",
      "сальск                    -10.2      21914\n",
      "средневековье             -10.2      23968\n",
      "алгебра (школьный предмет) -10.2        492\n",
      "завести, обзавестись      -10.2       6976\n",
      "среднеуральск             -10.2      23983\n",
      "славск                    -10.2      23011\n",
      "гидросамолет              -10.2       4558\n",
      "старшеклассник            -10.2      24122\n",
      "ум, интеллект             -10.2      26336\n",
      "диалект                   -10.2       5909\n",
      "голубь                    -10.2       4773\n",
      "казахский язык            -10.2       8716\n",
      "продовольствие            -10.2      19668\n",
      "валлийский язык           -10.2       2502\n",
      "баротравма                -10.2       1318\n",
      "навести преступника       -10.2      13266\n"
     ]
    }
   ],
   "source": [
    "for score, i in zip(vals.tolist(), indices.tolist()):\n",
    "    print(f'{hype_list[i]:25} {score:.3} {i:10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate bert's metrics on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40 hyponyms are not found in the index\n"
     ]
    }
   ],
   "source": [
    "train_hype_list = get_hypernyms_list_from_train(train_set_path)\n",
    "\n",
    "ds = HypoDataset(tokenizer,\n",
    "                 corpus_path,\n",
    "                 hypo_index_path,\n",
    "                 train_set_path,\n",
    "                 train_hype_list)\n",
    "\n",
    "corpus = ds.corpus\n",
    "index = ds.hypo_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedder import get_word_embeddings\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(model_path / 'bert_config.json')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=False)\n",
    "model = BertModel.from_pretrained(str(model_path / 'ptrubert.pt'), config=config)\n",
    "\n",
    "emb_mat = model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing /home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data/synsets.A.xml.\n",
      "Parsing /home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data/synsets.N.xml.\n",
      "Parsing /home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data/synsets.V.xml.\n",
      "Parsing /home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data/synset_relations.N.xml.\n",
      "Parsing /home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data/synset_relations.A.xml.\n",
      "Parsing /home/vimary/code-projects/dialog-2020-challenge/taxonomy-enrichment/data/synset_relations.V.xml.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_wordnet_synsets, enrich_with_wordnet_relations\n",
    "\n",
    "wordnet_synsets = get_wordnet_synsets(wordnet_path.glob('synsets.*'))\n",
    "enrich_with_wordnet_relations(wordnet_synsets, wordnet_path.glob('synset_relations.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernym embeddings are of shape torch.Size([29296, 768])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "LEVEL = 'sense'\n",
    "\n",
    "hypo2synset = collections.defaultdict(list)\n",
    "if LEVEL == 'synset':\n",
    "    # embedding all synsets (list of phrases)\n",
    "    hype_embs, hype_list = [], []\n",
    "    for s_id, synset in wordnet_synsets.items():\n",
    "        if s_id[-1] != 'N':\n",
    "            print(f'skipping {s_id}')\n",
    "        hype_list.append(synset['ruthes_name'].lower())\n",
    "        hypo2synset[hype_list[-1]].append(s_id)\n",
    "        senses = [sense['content'].lower() for sense in synset['senses']]\n",
    "        hype_embs.append(get_word_embeddings(senses, emb_mat, tokenizer=tokenizer).mean(dim=0))\n",
    "    hype_embs = torch.stack(hype_embs, dim=0)\n",
    "elif LEVEL == 'sense':\n",
    "    # embedding all senses (phrases)\n",
    "    hype_list = []\n",
    "    for s_id, synset in wordnet_synsets.items():\n",
    "        if s_id[-1] == 'N':\n",
    "            hype_list.append(synset['ruthes_name'].lower())\n",
    "            hypo2synset[hype_list[-1]].append(s_id)\n",
    "    hype_embs = get_word_embeddings(hype_list, emb_mat, tokenizer=tokenizer)\n",
    "print(f\"Hypernym embeddings are of shape {hype_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  5405,\n",
       "  9111,\n",
       "  29924,\n",
       "  22622,\n",
       "  40623,\n",
       "  128,\n",
       "  1997,\n",
       "  54560,\n",
       "  851,\n",
       "  34005,\n",
       "  98701,\n",
       "  2790,\n",
       "  108930,\n",
       "  132,\n",
       "  102]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_hypo(hypo: str, num_contexts: int = 1):\n",
    "    token_idxs, hypo_masks = [], []\n",
    "    max_len = 0\n",
    "\n",
    "    for i, (corp_id, start_idx, end_idx) in enumerate(index[hypo]):\n",
    "        text = corpus[corp_id]\n",
    "        if len(text.split()) > 250:\n",
    "            continue\n",
    "\n",
    "        subtokens, mask = ['[CLS]'], [0.0]\n",
    "        for n, token in enumerate(text.split()):\n",
    "            current_subtokens = tokenizer.tokenize(token)\n",
    "            subtokens.extend(current_subtokens)\n",
    "            mask_val = 1.0 if n in range(start_idx, end_idx) else 0.0\n",
    "            mask.extend([mask_val] * len(current_subtokens))\n",
    "        subtokens.append('[SEP]')\n",
    "#         print(sum(mask))\n",
    "        mask.append(0.0)\n",
    "\n",
    "        token_idxs.append(tokenizer.convert_tokens_to_ids(subtokens))\n",
    "        hypo_masks.append(mask)\n",
    "        max_len = max_len if max_len > len(subtokens) else len(subtokens)\n",
    "        if i >= num_contexts - 1:\n",
    "            break\n",
    "\n",
    "    # pad to max_len\n",
    "    attn_masks = [[1.0] * len(idxs) + [0.0] * (max_len - len(idxs)) for idxs in token_idxs]\n",
    "    token_idxs = [idxs + [0] * (max_len - len(idxs)) for idxs in token_idxs]\n",
    "    hypo_masks = [mask + [0.0] * (max_len - len(mask)) for mask in hypo_masks]\n",
    "    return token_idxs, hypo_masks, attn_masks\n",
    "\n",
    "encode_hypo('эпилепсия')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 \t 1.0\n",
      "0.009900990099009901 \t 0.31683168316831684\n",
      "0.03980099502487562 \t 0.40298507462686567\n",
      "0.03986710963455149 \t 0.38870431893687707\n",
      "0.0399002493765586 \t 0.40399002493765584\n",
      "0.03992015968063872 \t 0.4471057884231537\n",
      "0.048252911813643926 \t 0.4425956738768719\n",
      "0.04992867332382311 \t 0.44935805991440797\n",
      "0.052434456928838954 \t 0.4606741573033708\n",
      "0.05549389567147614 \t 0.46503884572697\n",
      "0.057942057942057944 \t 0.4675324675324675\n",
      "0.055404178019981834 \t 0.4641235240690282\n",
      "0.05661948376353039 \t 0.4612822647793505\n",
      "0.05534204458109147 \t 0.46887009992313605\n",
      "0.055674518201284794 \t 0.4725196288365453\n",
      "0.05263157894736842 \t 0.4776815456362425\n",
      "0.05121798875702686 \t 0.48282323547782635\n",
      "0.052322163433274546 \t 0.5008818342151675\n",
      "0.053303720155469185 \t 0.5047196002220988\n",
      "0.052603892688058915 \t 0.5023671751709626\n",
      "0.050474762618690654 \t 0.5077461269365318\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "METRIC = 'cosine'\n",
    "NUM_CONTEXTS = 1\n",
    "\n",
    "set_matches, set_matches_ext = [], []\n",
    "for i, (hypo, hypes, hype_hypes) in enumerate(ds.dataset):\n",
    "    if i % 100 == 1:\n",
    "        print(sum(set_matches) / len(set_matches), '\\t', sum(set_matches_ext) / len(set_matches_ext))\n",
    "    try:\n",
    "        token_idxs, hypo_masks, attn_masks = encode_hypo(hypo, num_contexts=NUM_CONTEXTS)\n",
    "        hypernyms = list(itertools.chain(*(hypes + hype_hypes)))\n",
    "\n",
    "        # h: [batch_size, seq_len, hidden_size]\n",
    "        h, v = model(torch.tensor(token_idxs), attention_mask=torch.tensor(attn_masks))\n",
    "    #     print('h norm =', h.norm(dim=2))\n",
    "        # hypo_mask_t: [batch_size, seq_len]\n",
    "        hypo_mask_t = torch.tensor(hypo_masks)\n",
    "        # r: [batch_size, seq_len, hidden_size]\n",
    "        r = h * hypo_mask_t.unsqueeze(2)\n",
    "        # r: [batch_size, hidden_size]\n",
    "        r = r.sum(dim=1) / hypo_mask_t.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # scores: [batch_size, vocab_size]\n",
    "        if METRIC == 'cosine':\n",
    "            # hype_embs_norm: [vocab_size, hidden_size]\n",
    "            hype_embs_norm = hype_embs / hype_embs.norm(dim=1, keepdim=True)\n",
    "            scores = r @ hype_embs_norm.T / r.norm(dim=1)\n",
    "        elif METRIC == 'product':\n",
    "            scores = r @ hype_embs.T\n",
    "\n",
    "        scores = torch.log_softmax(scores, dim=1)\n",
    "        # scores: [vocab_size]\n",
    "        scores = scores.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "        preds = sorted(zip(hype_list, scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "        pred_hypers = [(sense['content'].lower(), sc) for h, sc in preds for s_id in hypo2synset[h]\n",
    "                       for h_s_id in wordnet_synsets[s_id].get('hypernyms', [])\n",
    "                       for sense in wordnet_synsets[h_s_id['id']]['senses']]\n",
    "        set_matches.append(bool(set(p for p, sc in preds) & set(hypernyms)))\n",
    "        set_matches_ext.append(bool(set(p for p, sc in preds+pred_hypers) & set(hypernyms)))\n",
    "    except Exception as msg:\n",
    "        print('warning: ', msg)\n",
    "#     if set_matches[-1] != 0:\n",
    "#         print(hypo)\n",
    "#         print(hypernyms)\n",
    "#         print(set(p for p, sc in preds) & set(hypernyms))\n",
    "    \n",
    "print(sum(set_matches) / len(set_matches))\n",
    "print(sum(set_matches_ext) / len(set_matches_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05763473053892216\n",
      "2.5773453093812377\n"
     ]
    }
   ],
   "source": [
    "print(sum(set_matches) / len(set_matches))\n",
    "print(sum(set_matches_ext) / len(set_matches_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(set_matches_ext, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo, preds, pred_hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
