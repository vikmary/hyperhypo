{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from random import randint, sample\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from transformers.convert_bert_original_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "Save PyTorch model to /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/ptrubert.pt\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/bert_model.ckpt.index\n",
      "INFO:transformers.tokenization_utils:Model name '/home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2' is a path or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils:Didn't find file /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/special_tokens_map.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:Didn't find file /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/tokenizer_config.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils:loading file /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/vocab.txt\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n",
      "INFO:transformers.tokenization_utils:loading file None\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model_path = Path('/home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/')\n",
    "# model_path = Path('/home/mishanya/models/rubert_cased_L-12_H-768_A-12_v2/')\n",
    "convert_tf_checkpoint_to_pytorch(model_path / 'bert_model.ckpt.index', \n",
    "                                 model_path / 'bert_config.json',\n",
    "                                 model_path / 'ptrubert.pt')\n",
    "# config = BertConfig.from_pretrained(model_path / 'bert_config.json')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=False)\n",
    "# model = BertModel.from_pretrained(str(model_path / 'ptrubert.pt'), config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "class HypoDataset(IterableDataset):\n",
    "    def __init__(self,\n",
    "                 tokenizer: BertTokenizer,\n",
    "                 corpus_path: Union[str, Path],\n",
    "                 hypo_index_path: Union[str, Path],\n",
    "                 train_set_path: Union[str, Path]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.corpus = self._read_corpus(corpus_path)\n",
    "        self.hypo_index = self._read_json(hypo_index_path)\n",
    "        self.train_set = self._read_json(train_set_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_json(hypo_index_path: Union[str, Path]):\n",
    "        with open(hypo_index_path, encoding='utf8') as handle:\n",
    "            return json.load(handle)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _read_corpus(corpus_path: Union[str, Path]):\n",
    "        with open(corpus_path, encoding='utf8') as handle:\n",
    "            return handle.readlines()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            train_ind = randint(0, len(self.train_set) - 1)\n",
    "            hypos, hypes, hype_hypes = self.train_set[train_ind]\n",
    "            hypos_in_index = [h for h in hypos if h in self.hypo_index]\n",
    "            \n",
    "            if not hypos_in_index:\n",
    "                print(f'Empty index for hypos: {hypos}')\n",
    "                continue\n",
    "            if len(hypos) != len(hypos_in_index):\n",
    "                print(f'Some hypos are lost. Original: {hypos},'\n",
    "                      f' In index: {hypos_in_index }')\n",
    "                \n",
    "            hypo = sample(hypos_in_index, 1)[0]\n",
    "            sent_idx, inner_hypo_idx = sample(self.hypo_index[hypo], 1)[0]\n",
    "            sent_toks = self.corpus[sent_idx].split()\n",
    "            sent_subtok_idxs = []\n",
    "            subtokens_sent = []\n",
    "            hypo_mask = []\n",
    "            for n, tok in enumerate(sent_toks):\n",
    "                subtokens = self.tokenizer.tokenize(tok)\n",
    "                subtokens_sent.extend(subtokens)\n",
    "                subtok_idxs = self.tokenizer.convert_tokens_to_ids(subtokens)\n",
    "                sent_subtok_idxs.extend(subtok_idxs)\n",
    "                mask_value = float(n == inner_hypo_idx)\n",
    "                hypo_mask.extend([mask_value] * len(subtok_idxs))\n",
    "            \n",
    "            yield sent_subtok_idxs, subtokens_sent, hypo_mask\n",
    "            \n",
    "    def embed_hypernym(self, hypernyms: List[str]):\n",
    "        raise NotImplementedError  \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "data_path = Path('/home/arcady/data/hypernym/')\n",
    "# data_path = Path('/home/mishanya/data/hypernym/')\n",
    "ds = HypoDataset(tokenizer,\n",
    "                 data_path / 'tst_corpus.txt',\n",
    "                 data_path / 'tst_index.json',\n",
    "                 data_path / 'tst_train.json')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Some hypos are lost. Original: ['кот', 'кошара'], In index: ['кот']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "sti, st, m = next(iter(ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(['А',\n  'это',\n  'странный',\n  'пример',\n  'в',\n  'котором',\n  'кот',\n  '##ейка',\n  'не',\n  'участвует',\n  '.'],\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 32
    }
   ],
   "source": [
    "st, m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "class BertHypoHype(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_path: Union[str, Path],\n",
    "                 config_path: Union[str, Path]):\n",
    "        super(BertHypoHype, self).__init__()\n",
    "        config = BertConfig.from_pretrained(str(config_path))\n",
    "        self.bert = BertModel.from_pretrained(str(model_path), config=config)\n",
    "        \n",
    "    def forward(self, indices_batch: torch.LongTensor, hypo_mask: torch.Tensor):\n",
    "        h = self.bert(indices_batch)[0]\n",
    "        m = torch.tensor(hypo_mask).unsqueeze(2)\n",
    "        return torch.mean(h * m, 1)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "def batch_collate(batch):\n",
    "    indices, strings, mask = list(zip(*batch))\n",
    "    batch_size = len(indices)\n",
    "    max_len = max(len(idx) for idx in indices)\n",
    "    padded_indices = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    padded_mask = torch.zeros(batch_size, max_len, dtype=torch.float)\n",
    "    for n, (sent_idxs, sent_mask) in enumerate(zip(indices, mask)):\n",
    "        up_to = len(sent_idxs)\n",
    "        sent_idxs = torch.tensor(sent_idxs)\n",
    "        sent_mask = torch.tensor(sent_mask)\n",
    "        padded_indices[n, :up_to] = sent_idxs\n",
    "        padded_mask[n, :up_to] = sent_mask\n",
    "    return padded_indices, padded_mask\n",
    "    \n",
    "\n",
    "dl = DataLoader(ds, batch_size=2, collate_fn=batch_collate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Some hypos are lost. Original: ['кот', 'кошара'], In index: ['кот']\n",
      "Some hypos are lost. Original: ['кот', 'кошара'], In index: ['кот']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "idxs, mask = next(iter(dl))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/bert_config.json\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file /home/arcady/data/models/rubert_cased_L-12_H-768_A-12_v2/ptrubert.pt\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = BertHypoHype(model_path / 'ptrubert.pt',\n",
    "                     model_path / 'bert_config.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/arcady/.envs/env/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "tensor([[-0.0145,  0.0589, -0.0246,  ..., -0.0310, -0.0322, -0.0069],\n        [-0.0131,  0.0444,  0.0253,  ..., -0.0762, -0.1393, -0.0675]],\n       grad_fn=<MeanBackward1>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 37
    }
   ],
   "source": [
    "model(idxs, mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "['bert.embeddings.word_embeddings.weight',\n 'bert.embeddings.position_embeddings.weight',\n 'bert.embeddings.token_type_embeddings.weight',\n 'bert.embeddings.LayerNorm.weight',\n 'bert.embeddings.LayerNorm.bias',\n 'bert.encoder.layer.0.attention.self.query.weight',\n 'bert.encoder.layer.0.attention.self.query.bias',\n 'bert.encoder.layer.0.attention.self.key.weight',\n 'bert.encoder.layer.0.attention.self.key.bias',\n 'bert.encoder.layer.0.attention.self.value.weight',\n 'bert.encoder.layer.0.attention.self.value.bias',\n 'bert.encoder.layer.0.attention.output.dense.weight',\n 'bert.encoder.layer.0.attention.output.dense.bias',\n 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.0.intermediate.dense.weight',\n 'bert.encoder.layer.0.intermediate.dense.bias',\n 'bert.encoder.layer.0.output.dense.weight',\n 'bert.encoder.layer.0.output.dense.bias',\n 'bert.encoder.layer.0.output.LayerNorm.weight',\n 'bert.encoder.layer.0.output.LayerNorm.bias',\n 'bert.encoder.layer.1.attention.self.query.weight',\n 'bert.encoder.layer.1.attention.self.query.bias',\n 'bert.encoder.layer.1.attention.self.key.weight',\n 'bert.encoder.layer.1.attention.self.key.bias',\n 'bert.encoder.layer.1.attention.self.value.weight',\n 'bert.encoder.layer.1.attention.self.value.bias',\n 'bert.encoder.layer.1.attention.output.dense.weight',\n 'bert.encoder.layer.1.attention.output.dense.bias',\n 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.1.intermediate.dense.weight',\n 'bert.encoder.layer.1.intermediate.dense.bias',\n 'bert.encoder.layer.1.output.dense.weight',\n 'bert.encoder.layer.1.output.dense.bias',\n 'bert.encoder.layer.1.output.LayerNorm.weight',\n 'bert.encoder.layer.1.output.LayerNorm.bias',\n 'bert.encoder.layer.2.attention.self.query.weight',\n 'bert.encoder.layer.2.attention.self.query.bias',\n 'bert.encoder.layer.2.attention.self.key.weight',\n 'bert.encoder.layer.2.attention.self.key.bias',\n 'bert.encoder.layer.2.attention.self.value.weight',\n 'bert.encoder.layer.2.attention.self.value.bias',\n 'bert.encoder.layer.2.attention.output.dense.weight',\n 'bert.encoder.layer.2.attention.output.dense.bias',\n 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.2.intermediate.dense.weight',\n 'bert.encoder.layer.2.intermediate.dense.bias',\n 'bert.encoder.layer.2.output.dense.weight',\n 'bert.encoder.layer.2.output.dense.bias',\n 'bert.encoder.layer.2.output.LayerNorm.weight',\n 'bert.encoder.layer.2.output.LayerNorm.bias',\n 'bert.encoder.layer.3.attention.self.query.weight',\n 'bert.encoder.layer.3.attention.self.query.bias',\n 'bert.encoder.layer.3.attention.self.key.weight',\n 'bert.encoder.layer.3.attention.self.key.bias',\n 'bert.encoder.layer.3.attention.self.value.weight',\n 'bert.encoder.layer.3.attention.self.value.bias',\n 'bert.encoder.layer.3.attention.output.dense.weight',\n 'bert.encoder.layer.3.attention.output.dense.bias',\n 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.3.intermediate.dense.weight',\n 'bert.encoder.layer.3.intermediate.dense.bias',\n 'bert.encoder.layer.3.output.dense.weight',\n 'bert.encoder.layer.3.output.dense.bias',\n 'bert.encoder.layer.3.output.LayerNorm.weight',\n 'bert.encoder.layer.3.output.LayerNorm.bias',\n 'bert.encoder.layer.4.attention.self.query.weight',\n 'bert.encoder.layer.4.attention.self.query.bias',\n 'bert.encoder.layer.4.attention.self.key.weight',\n 'bert.encoder.layer.4.attention.self.key.bias',\n 'bert.encoder.layer.4.attention.self.value.weight',\n 'bert.encoder.layer.4.attention.self.value.bias',\n 'bert.encoder.layer.4.attention.output.dense.weight',\n 'bert.encoder.layer.4.attention.output.dense.bias',\n 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.4.intermediate.dense.weight',\n 'bert.encoder.layer.4.intermediate.dense.bias',\n 'bert.encoder.layer.4.output.dense.weight',\n 'bert.encoder.layer.4.output.dense.bias',\n 'bert.encoder.layer.4.output.LayerNorm.weight',\n 'bert.encoder.layer.4.output.LayerNorm.bias',\n 'bert.encoder.layer.5.attention.self.query.weight',\n 'bert.encoder.layer.5.attention.self.query.bias',\n 'bert.encoder.layer.5.attention.self.key.weight',\n 'bert.encoder.layer.5.attention.self.key.bias',\n 'bert.encoder.layer.5.attention.self.value.weight',\n 'bert.encoder.layer.5.attention.self.value.bias',\n 'bert.encoder.layer.5.attention.output.dense.weight',\n 'bert.encoder.layer.5.attention.output.dense.bias',\n 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.5.intermediate.dense.weight',\n 'bert.encoder.layer.5.intermediate.dense.bias',\n 'bert.encoder.layer.5.output.dense.weight',\n 'bert.encoder.layer.5.output.dense.bias',\n 'bert.encoder.layer.5.output.LayerNorm.weight',\n 'bert.encoder.layer.5.output.LayerNorm.bias',\n 'bert.encoder.layer.6.attention.self.query.weight',\n 'bert.encoder.layer.6.attention.self.query.bias',\n 'bert.encoder.layer.6.attention.self.key.weight',\n 'bert.encoder.layer.6.attention.self.key.bias',\n 'bert.encoder.layer.6.attention.self.value.weight',\n 'bert.encoder.layer.6.attention.self.value.bias',\n 'bert.encoder.layer.6.attention.output.dense.weight',\n 'bert.encoder.layer.6.attention.output.dense.bias',\n 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.6.intermediate.dense.weight',\n 'bert.encoder.layer.6.intermediate.dense.bias',\n 'bert.encoder.layer.6.output.dense.weight',\n 'bert.encoder.layer.6.output.dense.bias',\n 'bert.encoder.layer.6.output.LayerNorm.weight',\n 'bert.encoder.layer.6.output.LayerNorm.bias',\n 'bert.encoder.layer.7.attention.self.query.weight',\n 'bert.encoder.layer.7.attention.self.query.bias',\n 'bert.encoder.layer.7.attention.self.key.weight',\n 'bert.encoder.layer.7.attention.self.key.bias',\n 'bert.encoder.layer.7.attention.self.value.weight',\n 'bert.encoder.layer.7.attention.self.value.bias',\n 'bert.encoder.layer.7.attention.output.dense.weight',\n 'bert.encoder.layer.7.attention.output.dense.bias',\n 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.7.intermediate.dense.weight',\n 'bert.encoder.layer.7.intermediate.dense.bias',\n 'bert.encoder.layer.7.output.dense.weight',\n 'bert.encoder.layer.7.output.dense.bias',\n 'bert.encoder.layer.7.output.LayerNorm.weight',\n 'bert.encoder.layer.7.output.LayerNorm.bias',\n 'bert.encoder.layer.8.attention.self.query.weight',\n 'bert.encoder.layer.8.attention.self.query.bias',\n 'bert.encoder.layer.8.attention.self.key.weight',\n 'bert.encoder.layer.8.attention.self.key.bias',\n 'bert.encoder.layer.8.attention.self.value.weight',\n 'bert.encoder.layer.8.attention.self.value.bias',\n 'bert.encoder.layer.8.attention.output.dense.weight',\n 'bert.encoder.layer.8.attention.output.dense.bias',\n 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.8.intermediate.dense.weight',\n 'bert.encoder.layer.8.intermediate.dense.bias',\n 'bert.encoder.layer.8.output.dense.weight',\n 'bert.encoder.layer.8.output.dense.bias',\n 'bert.encoder.layer.8.output.LayerNorm.weight',\n 'bert.encoder.layer.8.output.LayerNorm.bias',\n 'bert.encoder.layer.9.attention.self.query.weight',\n 'bert.encoder.layer.9.attention.self.query.bias',\n 'bert.encoder.layer.9.attention.self.key.weight',\n 'bert.encoder.layer.9.attention.self.key.bias',\n 'bert.encoder.layer.9.attention.self.value.weight',\n 'bert.encoder.layer.9.attention.self.value.bias',\n 'bert.encoder.layer.9.attention.output.dense.weight',\n 'bert.encoder.layer.9.attention.output.dense.bias',\n 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.9.intermediate.dense.weight',\n 'bert.encoder.layer.9.intermediate.dense.bias',\n 'bert.encoder.layer.9.output.dense.weight',\n 'bert.encoder.layer.9.output.dense.bias',\n 'bert.encoder.layer.9.output.LayerNorm.weight',\n 'bert.encoder.layer.9.output.LayerNorm.bias',\n 'bert.encoder.layer.10.attention.self.query.weight',\n 'bert.encoder.layer.10.attention.self.query.bias',\n 'bert.encoder.layer.10.attention.self.key.weight',\n 'bert.encoder.layer.10.attention.self.key.bias',\n 'bert.encoder.layer.10.attention.self.value.weight',\n 'bert.encoder.layer.10.attention.self.value.bias',\n 'bert.encoder.layer.10.attention.output.dense.weight',\n 'bert.encoder.layer.10.attention.output.dense.bias',\n 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.10.intermediate.dense.weight',\n 'bert.encoder.layer.10.intermediate.dense.bias',\n 'bert.encoder.layer.10.output.dense.weight',\n 'bert.encoder.layer.10.output.dense.bias',\n 'bert.encoder.layer.10.output.LayerNorm.weight',\n 'bert.encoder.layer.10.output.LayerNorm.bias',\n 'bert.encoder.layer.11.attention.self.query.weight',\n 'bert.encoder.layer.11.attention.self.query.bias',\n 'bert.encoder.layer.11.attention.self.key.weight',\n 'bert.encoder.layer.11.attention.self.key.bias',\n 'bert.encoder.layer.11.attention.self.value.weight',\n 'bert.encoder.layer.11.attention.self.value.bias',\n 'bert.encoder.layer.11.attention.output.dense.weight',\n 'bert.encoder.layer.11.attention.output.dense.bias',\n 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.11.intermediate.dense.weight',\n 'bert.encoder.layer.11.intermediate.dense.bias',\n 'bert.encoder.layer.11.output.dense.weight',\n 'bert.encoder.layer.11.output.dense.bias',\n 'bert.encoder.layer.11.output.LayerNorm.weight',\n 'bert.encoder.layer.11.output.LayerNorm.bias',\n 'bert.pooler.dense.weight',\n 'bert.pooler.dense.bias']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 38
    }
   ],
   "source": [
    "list(model.state_dict())\n",
    "# torch.optim.Adam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    eps: 1e-08\n    lr: 1e-05\n    weight_decay: 0\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 39
    }
   ],
   "source": [
    "torch.optim.Adam(model.bert.encoder.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BertTokenizerFast('vocab.txt', do_lower_case=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'/home/mishanya/data/hypernym/candidates.tsv'\n",
    "\n",
    "tokenizer = BertTokenizer('vocab.txt', do_lower_case=False)\n",
    "token_lengths = Counter()\n",
    "prefix_index = defaultdict(set)\n",
    "description_to_synset = {}\n",
    "\n",
    "# with open('/home/arcady/data/hypernym/candidates.tsv') as handle:\n",
    "with open('/home/mishanya/data/hypernym/candidates.tsv') as handle:\n",
    "    for line in tqdm(handle):\n",
    "        synset_id, description, tokens_str = line.split('\\t')\n",
    "        tokens = tokens_str.strip().lower().split(', ')\n",
    "        subtokens = [tokenizer.tokenize(tok) for tok in tokens]\n",
    "        token_lengths.update(len(st) for st in subtokens)\n",
    "        description_to_synset[description] = tokens\n",
    "        for st in subtokens:\n",
    "            prefix_index[st[0]].add(description)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_index = sorted(prefix_index.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "lens = [len(vals) for descript, vals in sorted_index]\n",
    "\n",
    "plt.plot(lens)\n",
    "plt.yscale('log')\n",
    "plt.grid()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "description_to_synset['БЫТОВКА ДЛЯ РАБОЧИХ']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jupytsorted_index[100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.tokenize('мвд')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '/home/arcady/data/hypernym/train.json'\n",
    "p = '/home/arcady/data/'\n",
    "with open(path) as handle:\n",
    "    data = json.load(handle)\n",
    "\n",
    "with open(path, 'w', encoding='utf-8') as handle:\n",
    "    json.dump(data, handle, indent=4, ensure_ascii=False)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "Counter(len(q.split(' ')) for q in chain(*[item[0] for item in data]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}